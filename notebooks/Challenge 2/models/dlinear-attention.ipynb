{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7164245,"sourceType":"datasetVersion","datasetId":4138370},{"sourceId":7169244,"sourceType":"datasetVersion","datasetId":4141950}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Fix randomness and hide warnings\nseed = 69\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=Warning)\nimport numpy as np\nnp.random.seed(seed)\nfrom scipy.interpolate import interp1d\nimport logging\nimport gc\nimport random\nrandom.seed(seed)\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nplt.rc('font', size=16)\n#from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as pyplot\n# Import tensorflow\nimport tensorflow as tf\nfrom tensorflow import keras as tfk\nfrom tensorflow.keras import layers as tfkl\ntf.autograph.set_verbosity(0)\ntf.get_logger().setLevel(logging.ERROR)\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)\nprint(tf.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loading - Preprocessing","metadata":{}},{"cell_type":"code","source":"telescope = 18\nbatch_size = 64\nepochs = 200\nwindow = 200\ninterpolated_window = 400\nstride = 10","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = np.load(\"/kaggle/input/time-series-anndl/training_data.npy\")\ncategories = np.load(\"/kaggle/input/time-series-anndl/categories.npy\")\nvalid_periods = np.load(\"/kaggle/input/time-series-anndl/valid_periods.npy\")\ndata.shape, categories.shape, valid_periods.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_sequences_filtered(target_data, valid_periods, window=200, stride=50, telescope=18):\n    assert window % stride == 0\n    outlier_detection_length = 50\n    delta = 0.005\n    outliers = []\n    dataset = []\n    labels = []\n    for i, signal in enumerate(target_data):\n        for j in np.arange(min(valid_periods[i][0],len(signal)-window-telescope),len(signal)-window-telescope,stride):\n            input_sequence = signal[j:j+window]#.astype(np.float32)\n            output_sequence = signal[j+window:j+window+telescope]#.astype(np.float32)\n            grad_less_delta = np.abs(np.gradient(input_sequence)) < delta\n            #Now we have an array of 0s and 1s\n            temp = np.diff(np.where(np.concatenate(([grad_less_delta[0]],grad_less_delta[:-1] != grad_less_delta[1:],[True])))[0])[::2]\n            #If temp contains at least one value that is greater than outlier_detection_length, then we have an outlier\n            if np.any(temp > outlier_detection_length):\n                outliers.append(i)\n            else:\n                dataset.append(input_sequence)\n                labels.append(output_sequence)\n    print(len(outliers))    \n    return np.expand_dims(np.array(dataset),axis=-1), np.expand_dims(np.array(labels),axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X, y = build_sequences_filtered(data,valid_periods,window=200,stride=stride,telescope=telescope)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\nX.shape, y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model definition","metadata":{}},{"cell_type":"code","source":"class DLinear(tfk.models.Model):\n    def __init__(self, output_shape, separate_features=False, kernel_size=25, num_attention_heads=4, cnn_filters=64, cnn_kernel_size=40, **kwargs):\n        super(DLinear, self).__init__(**kwargs)\n        self.kernel_size = kernel_size\n        self.num_attention_heads = num_attention_heads\n        self.output_steps = output_shape[0]\n        self.output_features = output_shape[1]\n        self.separate_features = separate_features\n        self.cnn_filters = cnn_filters\n        self.cnn_kernel_size = cnn_kernel_size\n        self.kernel_initializer = \"he_normal\"\n\n    def build(self, input_shape):\n        self.built_input_shape = input_shape\n        self.trend_cnn = tfkl.Conv1D(filters=self.cnn_filters, kernel_size=self.cnn_kernel_size, activation='relu', padding='same', kernel_initializer=self.kernel_initializer, name=\"trend_cnn\")\n        self.res_cnn = tfkl.Conv1D(filters=self.cnn_filters, kernel_size=self.cnn_kernel_size, activation='relu', padding='same', kernel_initializer=self.kernel_initializer, name=\"res_cnn\")\n        self.residual_dense = tfkl.Dense(self.output_steps * self.output_features, kernel_initializer=self.kernel_initializer, name=\"residual_recomposer\")\n        self.trend_output_dense = tfkl.Dense(self.output_steps * self.output_features, kernel_initializer=self.kernel_initializer, name=\"trend_output_dense\")\n        self.attention = tfkl.MultiHeadAttention(num_heads=self.num_attention_heads, key_dim=self.output_features)\n        super(DLinear, self).build(input_shape)\n\n    def call(self, inputs):\n        trend = tfkl.AveragePooling1D(pool_size=self.kernel_size, strides=1, padding=\"same\", name=\"trend_decomposer\")(inputs)\n        residual = tfkl.Subtract(name=\"residual_decomposer\")([inputs, trend])\n\n        # Apply CNN to the trend\n        trend_cnn_output = self.trend_cnn(trend)\n        res_cnn_output = self.res_cnn(residual)\n        \n        trend_attended = self.attention(trend_cnn_output, trend_cnn_output)\n        #res_attended = self.attention(res_cnn_output, res_cnn_output)\n\n        flat_residual = tfkl.Flatten()(res_cnn_output)\n        flat_trend = tfkl.Flatten()(trend_attended)\n        \n        residual = self.residual_dense(flat_residual)\n        trend = self.trend_output_dense(flat_trend)\n        add = tfkl.Add(name=\"recomposer\")([residual, trend])\n        \n        reshape = tfkl.Reshape((self.output_steps, self.output_features))(add)\n        return reshape\n\n    def summary(self):\n        if self.built:\n            self.model().summary()\n        else:\n            super().summary()\n\n    def model(self):\n        x = tfkl.Input(shape=(self.built_input_shape[1:]))\n        model = tfk.models.Model(inputs=[x], outputs=self.call(x))\n        return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DLinear((telescope,1), separate_features=False, kernel_size=25)\nmodel.build((None,window,1))\nmodel.compile(loss=tf.keras.losses.MeanSquaredError(), metrics=['mae'], optimizer=tf.keras.optimizers.AdamW(1e-3))\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def make_dataset(X, y, batch_size=128, prefetch_amt=tf.data.experimental.AUTOTUNE):\n    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n    # reshape x from (200) to (200,1) and y from (18) to (18,1)\n    dataset = dataset.map(lambda x, y: (tf.reshape(x, (200,1)), tf.reshape(y, (18,1))),num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch_size, drop_remainder=False)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(prefetch_amt)\n    return dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n    make_dataset(X_train, np.squeeze(y_train,axis=-1), batch_size),\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_data=make_dataset(X_val,np.squeeze(y_val,axis=-1), batch_size),\n    callbacks = [\n        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=5, min_delta=5e-6, restore_best_weights=True),\n        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=4, factor=0.1)\n    ],\n).history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_val,y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_epoch = np.argmin(history['val_loss'])\nplt.figure(figsize=(17,4))\nplt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Squared Error (Loss)')\nplt.legend()\nplt.grid(alpha=.3)\nplt.ylim([0,0.02])\nplt.show()\n\nplt.figure(figsize=(18,3))\nplt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('DLinear_tel_18_win200_att')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r DLinear_Att.zip /kaggle/working/DLinear_tel_18_win200_att","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'DLinear_Att.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}